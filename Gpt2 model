from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
import torch

# Load tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Read and preprocess the dataset
with open("your_dataset.txt", "r", encoding="utf-8") as file:
    data = file.readlines()

# Manually pad sequences to a fixed length
max_length = 512
padded_data = [text[:max_length] + [tokenizer.pad_token_id] * (max_length - len(text)) for text in tokenizer(data)["input_ids"]]

# Convert padded data to PyTorch tensor
input_ids = torch.tensor(padded_data)

# Fine-tune the model
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Define training arguments
training_args = TrainingArguments(
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=1000,
    save_steps=2000,
    save_total_limit=2,
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=input_ids,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained("fine_tuned_model")
