training_args = TrainingArguments(
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=1000,
    save_steps=2000,
    save_total_limit=2,
    warmup_steps=500,  # Adjust the number of warmup steps
    weight_decay=0.01,  # Adjust weight decay (L2 regularization)
    learning_rate=5e-5,  # Adjust learning rate
    adam_epsilon=1e-8,  # Adjust epsilon parameter for Adam optimizer
    max_steps=-1,  # Adjust maximum number of training steps
)
